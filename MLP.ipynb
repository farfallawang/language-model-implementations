{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5), ('f', 6), ('g', 7), ('h', 8), ('i', 9), ('j', 10), ('k', 11), ('l', 12), ('m', 13), ('n', 14), ('o', 15), ('p', 16), ('q', 17), ('r', 18), ('s', 19), ('t', 20), ('u', 21), ('v', 22), ('w', 23), ('x', 24), ('y', 25), ('z', 26), ('.', 0)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(letters)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "stoi.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a one-pass forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 13, 13, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Instantiate X\n",
    "\n",
    "input = '.emma'\n",
    "input_vec = []\n",
    "for ch in input:\n",
    "    input_vec.append(stoi[ch])\n",
    "input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate W matrix\n",
    "\n",
    "cl = 27\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand([cl, cl], generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "## Compute forward pass \n",
    "X = torch.tensor(input_vec)\n",
    "xenc = F.one_hot(X, num_classes=cl).float() #input to neural net: one-hot encoding\n",
    "y = xenc @ W    # predict log counts can also use torch.matmul(xenc, W)\n",
    "\n",
    "#### Calculate probability matrix \n",
    "cnt = torch.exp(y)  #counts, equivalent to N\n",
    "P = cnt / cnt.sum(dim=1, keepdim=True) # probability matrix for next character\n",
    "print(P.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.02 Log probability: -3.78 Negative log likelihood (i.e. loss): 3.78\n",
      "Probability: 0.03 Log probability: -3.66 Negative log likelihood (i.e. loss): 3.66\n",
      "Probability: 0.03 Log probability: -3.38 Negative log likelihood (i.e. loss): 3.38\n",
      "Probability: 0.03 Log probability: -3.38 Negative log likelihood (i.e. loss): 3.38\n",
      "Probability: 0.04 Log probability: -3.21 Negative log likelihood (i.e. loss): 3.21\n",
      "Mean loss: 3.48\n"
     ]
    }
   ],
   "source": [
    "## Instantiate y\n",
    "\n",
    "label = 'emma.'\n",
    "ys = torch.tensor([stoi[chr] for chr in label])\n",
    "\n",
    "## Calculate loss (negative log-likelihood) for y\n",
    "loss = torch.tensor(0.0)\n",
    "cnt = 0\n",
    "for y in ys:\n",
    "    p = P[cnt, y]\n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    loss += nll\n",
    "    print('Probability:', round(p.item(), 2), 'Log probability:', round(logp.item(), 2), 'Negative log likelihood (i.e. loss):', round(nll.item(), 2))\n",
    "    cnt += 1\n",
    "\n",
    "mean_loss = loss / cnt\n",
    "print('Mean loss:', round(mean_loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0426, 0.0299, 0.0233, 0.0382, 0.0229, 0.0229, 0.0507, 0.0563, 0.0211,\n",
      "        0.0280, 0.0440, 0.0313, 0.0497, 0.0439, 0.0308, 0.0261, 0.0424, 0.0563,\n",
      "        0.0547, 0.0325, 0.0425, 0.0222, 0.0472, 0.0285, 0.0566, 0.0295, 0.0258])\n",
      "label (actual next character:) 5\n",
      "probability assigned by the net to the correct character 0.022935139015316963\n",
      "log likelihood: -3.78\n",
      "negative log likelihood: 3.78\n",
      "------------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0298, 0.0278, 0.0241, 0.0308, 0.0300, 0.0362, 0.0401, 0.0269, 0.0538,\n",
      "        0.0445, 0.0560, 0.0239, 0.0358, 0.0258, 0.0370, 0.0542, 0.0275, 0.0513,\n",
      "        0.0263, 0.0253, 0.0315, 0.0294, 0.0317, 0.0507, 0.0554, 0.0562, 0.0380])\n",
      "label (actual next character:) 13\n",
      "probability assigned by the net to the correct character 0.025837397202849388\n",
      "log likelihood: -3.66\n",
      "negative log likelihood: 3.66\n",
      "------------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0317, 0.0341, 0.0391, 0.0342, 0.0347, 0.0217, 0.0243, 0.0281, 0.0464,\n",
      "        0.0537, 0.0481, 0.0261, 0.0520, 0.0339, 0.0449, 0.0443, 0.0228, 0.0305,\n",
      "        0.0345, 0.0543, 0.0219, 0.0490, 0.0287, 0.0234, 0.0548, 0.0312, 0.0516])\n",
      "label (actual next character:) 13\n",
      "probability assigned by the net to the correct character 0.033900048583745956\n",
      "log likelihood: -3.38\n",
      "negative log likelihood: 3.38\n",
      "------------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0317, 0.0341, 0.0391, 0.0342, 0.0347, 0.0217, 0.0243, 0.0281, 0.0464,\n",
      "        0.0537, 0.0481, 0.0261, 0.0520, 0.0339, 0.0449, 0.0443, 0.0228, 0.0305,\n",
      "        0.0345, 0.0543, 0.0219, 0.0490, 0.0287, 0.0234, 0.0548, 0.0312, 0.0516])\n",
      "label (actual next character:) 1\n",
      "probability assigned by the net to the correct character 0.03406135365366936\n",
      "log likelihood: -3.38\n",
      "negative log likelihood: 3.38\n",
      "------------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0402, 0.0517, 0.0303, 0.0400, 0.0258, 0.0318, 0.0394, 0.0301, 0.0308,\n",
      "        0.0550, 0.0468, 0.0402, 0.0388, 0.0468, 0.0580, 0.0386, 0.0341, 0.0400,\n",
      "        0.0246, 0.0336, 0.0232, 0.0235, 0.0490, 0.0309, 0.0350, 0.0253, 0.0363])\n",
      "label (actual next character:) 0\n",
      "probability assigned by the net to the correct character 0.04024980217218399\n",
      "log likelihood: -3.21\n",
      "negative log likelihood: 3.21\n",
      "===========\n",
      "average neg log likelihood, i.e. loss = 3.4815192222595215\n"
     ]
    }
   ],
   "source": [
    "# Andrej's code\n",
    "xs = torch.tensor([0, 5, 13, 13, 1])\n",
    "ys = torch.tensor([5, 13, 13, 1, 0])\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    #i-th bigram:\n",
    "    x = xs[i].item() #input character index\n",
    "    y = ys[i].item() #input character index\n",
    "    print('------------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', P[i])\n",
    "    print('label (actual next character:)', y)\n",
    "    p = P[i, y]\n",
    "    print('probability assigned by the net to the correct character', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', round(logp.item(), 2))\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', round(nll.item(), 2))\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('===========')\n",
    "print('average neg log likelihood, i.e. loss =', round(nlls.mean().item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4815)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rewrite loss in vector form\n",
    "logp = P[torch.arange(5), ys].log().mean()\n",
    "loss = -logp\n",
    "\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = '.emma'\n",
    "xs = torch.tensor([stoi[chr] for chr in input])\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'emma.'\n",
    "ys = torch.tensor([stoi[chr] for chr in label])\n",
    "\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiat the W matrix\n",
    "cl = 27\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(cl, cl, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nll (i.e. loss): 3.77\n"
     ]
    }
   ],
   "source": [
    "## Compute forward pass \n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "y = xenc @ W    #calculate log counts can also use torch.matmul(xenc, W)\n",
    "\n",
    "## Calculate probability matrix \n",
    "cnt = torch.exp(y)  \n",
    "P = cnt / cnt.sum(dim=1, keepdim=True)  #probability matrix for next character \n",
    "\n",
    "## Calculate vectorized loss\n",
    "logp = P[torch.arange(5), ys].log().mean()\n",
    "loss = -logp\n",
    "\n",
    "print('Mean nll (i.e. loss):', round(loss.item(), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute backward pass \n",
    "delta = 0.1\n",
    "W.grad = None # set gradient to zero\n",
    "loss.backward() # will automatically calculates gradients\n",
    "\n",
    "W.data += -delta * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5661e+00, -2.3749e-01, -2.7631e-02, -1.1009e+00,  2.8554e-01,\n",
       "         -9.8891e-03, -1.5471e+00,  6.0443e-01,  7.8862e-02,  9.0400e-01,\n",
       "         -4.7141e-01,  7.8627e-01, -3.2862e-01, -4.3313e-01,  1.3719e+00,\n",
       "          2.9286e+00,  1.5606e+00, -1.6261e+00,  6.7666e-01, -8.4050e-01,\n",
       "          9.8420e-01, -1.4859e-01, -1.4796e+00,  4.4790e-01, -7.0966e-02,\n",
       "          2.4937e+00,  2.4419e+00],\n",
       "        [-6.5036e-01, -1.2201e+00,  3.0235e-01, -1.0727e+00,  7.2641e-01,\n",
       "          5.0498e-02,  1.3073e+00, -8.0246e-01, -8.5067e-01, -1.8069e+00,\n",
       "          1.2503e+00, -1.2258e+00,  1.2145e+00, -9.6500e-01, -2.3257e-01,\n",
       "         -3.4803e-01,  3.3163e-01, -1.3264e+00,  1.1206e+00,  5.9535e-01,\n",
       "          4.5753e-01,  5.3393e-02, -1.7401e+00,  1.1494e-01,  8.0189e-01,\n",
       "          5.4007e-01, -1.1648e+00],\n",
       "        [ 1.4756e-01, -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01,\n",
       "         -7.8305e-01,  1.3506e-01, -2.1161e-01, -1.0406e+00, -1.5367e+00,\n",
       "          9.3743e-01, -8.8303e-01,  1.7457e+00,  2.1346e+00, -8.5614e-01,\n",
       "          5.4082e-01,  6.1690e-01,  1.5160e+00, -1.0447e+00, -6.6414e-01,\n",
       "         -7.2390e-01,  1.7507e+00,  1.7530e-01,  9.9280e-01, -6.2787e-01,\n",
       "          7.7023e-02, -1.1641e+00],\n",
       "        [ 1.2473e+00, -2.7061e-01, -1.3635e+00,  1.3066e+00,  3.2307e-01,\n",
       "          1.0358e+00, -8.6249e-01, -1.2575e+00,  9.4180e-01, -1.3257e+00,\n",
       "          1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,  1.1491e+00,\n",
       "         -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01, -1.0188e+00,\n",
       "         -8.3777e-01, -2.1057e+00, -2.6044e-01, -1.7149e+00, -3.3787e-01,\n",
       "         -1.8263e+00, -8.3897e-01],\n",
       "        [-1.5723e+00,  4.5795e-01, -5.6533e-01,  5.4281e-01,  1.7549e-01,\n",
       "         -2.2901e+00, -7.0928e-01, -2.9283e-01, -2.1803e+00,  7.9311e-02,\n",
       "          9.0187e-01,  1.2028e+00, -5.6144e-01, -1.3753e-01, -1.3799e-01,\n",
       "         -2.0977e+00, -7.9238e-01,  6.0689e-01, -1.4777e+00, -5.1029e-01,\n",
       "          5.6421e-01,  9.6838e-01, -3.1114e-01, -3.0603e-01, -1.7495e+00,\n",
       "         -1.6335e+00,  3.8761e-01],\n",
       "        [ 4.7178e-01,  1.4814e+00,  3.1698e-01,  1.0578e+00,  2.3942e+00,\n",
       "          4.6769e-01, -6.5669e-01,  6.1595e-01, -6.2217e-01,  5.0947e-01,\n",
       "          1.3549e+00,  2.3399e-01, -4.5608e-01,  1.8326e-02, -5.1183e-01,\n",
       "          5.5507e-01,  4.7400e-01, -1.3868e+00,  1.6211e+00,  1.7154e-01,\n",
       "          9.8749e-01,  5.0597e-01,  1.0188e+00, -1.9063e+00, -4.2777e-01,\n",
       "         -2.1259e+00,  9.5946e-01],\n",
       "        [ 1.2482e+00,  2.5341e-01,  2.8188e+00, -3.3983e-01,  7.0311e-01,\n",
       "          4.0716e-01, -1.9018e-01, -6.9652e-01,  1.7039e+00,  7.4204e-01,\n",
       "          9.7370e-01,  3.0028e-01, -2.8971e-01, -3.1566e-01, -8.7898e-01,\n",
       "          1.0661e-01,  1.8598e+00,  5.5752e-02,  1.2815e+00, -6.3182e-01,\n",
       "         -1.2464e+00,  6.8305e-01, -3.9455e-01,  1.4388e-02,  5.7216e-01,\n",
       "          8.6726e-01,  6.3149e-01],\n",
       "        [-1.2230e+00, -2.1286e-01,  5.0950e-01,  3.2713e-01,  1.9661e+00,\n",
       "         -2.4091e-01, -7.9515e-01,  2.7198e-01, -1.1100e+00, -4.5285e-01,\n",
       "         -4.9578e-01,  1.2648e+00,  1.4625e+00,  1.1199e+00,  9.9539e-01,\n",
       "         -1.2353e+00,  7.3818e-01,  8.1415e-01, -7.3806e-01,  5.6714e-01,\n",
       "         -1.4601e+00, -2.4780e-01,  8.8282e-01, -8.1004e-02, -9.5299e-01,\n",
       "         -4.8838e-01, -7.3712e-01],\n",
       "        [ 7.0609e-01, -1.9295e-01,  1.2348e+00,  3.3308e-01,  1.3283e+00,\n",
       "         -1.0921e+00, -8.3952e-01,  1.9098e-01, -7.1750e-01, -3.8668e-01,\n",
       "         -1.2542e+00,  1.2068e+00, -1.7102e+00, -4.7701e-01, -1.0527e+00,\n",
       "         -1.4367e-01, -2.7737e-01,  1.1634e+00, -6.6910e-01,  6.4918e-01,\n",
       "          5.8243e-01,  1.9264e+00, -3.7846e-01,  7.9577e-03,  5.1068e-01,\n",
       "          7.5927e-01, -1.6086e+00],\n",
       "        [-1.6065e-01,  1.3784e+00, -2.7804e-01,  2.0710e-01,  1.0033e+00,\n",
       "         -5.9772e-01, -3.9771e-01, -1.2801e+00,  9.2445e-02,  1.0526e-01,\n",
       "         -3.9072e-01, -4.0091e-01,  5.6533e-01, -1.5065e+00,  1.2898e+00,\n",
       "         -1.5100e+00,  1.0930e+00,  1.0797e+00, -8.6681e-02,  1.3423e+00,\n",
       "          1.5184e-01,  2.4687e-01,  3.1895e-01, -9.8614e-01, -2.1382e-01,\n",
       "         -6.4308e-02, -8.5528e-01],\n",
       "        [ 1.6113e-01,  4.4925e-01,  8.1827e-01, -8.1628e-01, -3.9243e-01,\n",
       "         -7.4521e-01, -9.4649e-01, -1.5941e-01, -1.5047e+00,  8.4682e-01,\n",
       "         -4.9158e-02,  9.3866e-02, -6.4533e-01,  1.2108e+00, -7.8198e-01,\n",
       "          3.8449e-01, -8.5259e-01,  1.0464e+00, -1.8493e+00,  9.1092e-01,\n",
       "         -9.9360e-01,  6.0195e-01, -1.0890e-01,  5.2587e-01, -9.4046e-01,\n",
       "         -1.2773e-01, -2.5679e-01],\n",
       "        [-1.5437e+00,  3.7950e-01, -1.7705e+00, -1.2085e+00,  9.4773e-01,\n",
       "         -9.1355e-01,  7.1023e-01,  7.9512e-01,  5.7662e-01, -7.3778e-01,\n",
       "         -1.5264e+00,  7.1173e-01,  1.4056e+00, -4.0636e-01, -7.4648e-01,\n",
       "          4.9790e-01,  1.1298e-01, -4.1854e-01,  1.7905e-01,  2.3483e-01,\n",
       "          7.3510e-01, -6.1577e-01,  7.0467e-01,  1.1630e-01,  2.8365e-01,\n",
       "         -2.5043e+00, -5.1931e-01],\n",
       "        [-5.9134e-01, -1.1059e-01,  8.3416e-01, -1.0505e+00,  3.6345e-01,\n",
       "          1.8195e-01, -4.8045e-01,  5.3309e-01,  6.7869e-01, -3.5974e-01,\n",
       "         -1.3270e+00, -8.2526e-01,  6.3614e-01,  1.9110e-01,  7.5476e-01,\n",
       "          4.0538e-01,  2.2565e+00,  1.3655e+00, -5.6192e-01, -3.0423e-01,\n",
       "          2.9894e-01,  1.8784e+00,  5.5958e-01,  1.3388e+00,  4.1606e-01,\n",
       "          6.8491e-01, -1.4790e-01],\n",
       "        [ 1.9234e-01,  1.0703e+00,  6.3199e-01,  2.5653e-01,  9.6138e-01,\n",
       "         -2.4935e-01,  2.3702e-02, -3.1401e-02,  1.5573e+00, -4.4917e-01,\n",
       "         -1.2348e+00,  1.1188e+00, -6.7433e-01,  5.6815e-02, -5.5940e-01,\n",
       "         -8.2754e-01,  8.2019e-01, -7.5148e-01,  9.2518e-01, -1.4851e+00,\n",
       "         -2.1376e-01, -1.1864e+00, -6.6145e-01, -2.3429e-01,  1.5399e+00,\n",
       "          5.9873e-01, -7.0959e-01],\n",
       "        [ 1.9217e+00, -1.8182e-01,  1.5220e+00,  5.4644e-01,  4.0858e-01,\n",
       "         -1.9692e+00, -8.9185e-01,  3.2961e-01, -2.5128e-01,  5.5030e-01,\n",
       "         -7.5171e-01, -6.5783e-03, -6.3108e-01,  1.3431e+00,  3.8010e-02,\n",
       "         -7.1654e-01,  1.7206e+00, -5.2149e-01, -2.3248e-01,  1.0774e+00,\n",
       "         -7.6019e-01,  9.0109e-03, -7.9219e-01,  1.2307e+00, -5.2760e-01,\n",
       "         -1.3207e+00, -7.0654e-01],\n",
       "        [-7.7861e-01,  1.2910e+00, -1.5094e+00,  7.4593e-01,  4.8990e-01,\n",
       "         -1.0034e+00,  9.6407e-01,  2.0990e+00, -3.9870e-01, -7.6635e-01,\n",
       "         -2.1007e+00,  1.2331e+00,  7.7481e-01,  2.4311e-01, -2.1322e-01,\n",
       "         -6.9877e-01,  2.0889e-01, -6.2477e-01, -1.0825e-01, -2.1964e+00,\n",
       "          2.7083e-01,  6.1047e-01, -5.8162e-01, -1.7025e+00, -8.0672e-01,\n",
       "         -2.4174e-01,  1.5490e+00],\n",
       "        [-3.4593e-01,  5.4714e-01,  3.1755e-02,  8.1375e-01,  2.6200e-01,\n",
       "         -6.7101e-01,  2.0656e-02,  7.1300e-01, -4.3997e-02, -5.1944e-01,\n",
       "          1.1241e-01, -3.9770e-01, -2.7829e-01, -1.5364e-01, -2.5424e+00,\n",
       "          2.5033e-01,  1.1056e-01, -2.0366e+00, -9.2735e-01, -6.9350e-01,\n",
       "         -5.2788e-01, -8.7438e-01, -1.0102e+00, -1.0522e+00,  1.2348e+00,\n",
       "          2.5907e-02, -9.6676e-01],\n",
       "        [ 1.0904e+00,  5.3966e-01,  6.6741e-01, -2.2316e+00, -1.1603e+00,\n",
       "         -4.2560e-01,  5.9547e-01, -1.0887e+00,  2.4324e-01, -2.1021e+00,\n",
       "         -2.9289e-01, -7.0682e-01,  9.5190e-01, -1.1583e+00, -1.2844e+00,\n",
       "          1.0193e+00,  1.6851e+00,  8.3422e-01,  1.7113e+00,  4.4456e-01,\n",
       "         -7.1861e-01, -7.0343e-01, -7.1332e-01,  9.9760e-01, -6.1980e-01,\n",
       "          1.9522e+00,  1.4311e-01],\n",
       "        [ 1.8765e-01,  7.5974e-01, -2.6387e-01, -7.3048e-01,  6.1955e-01,\n",
       "          3.5577e-02, -7.6459e-02, -1.2306e+00,  1.3419e+00,  1.1878e+00,\n",
       "         -1.0672e+00, -2.1507e+00,  6.7082e-01,  1.1614e+00, -2.4155e-01,\n",
       "          9.5907e-01,  3.8262e-02,  3.9877e-02, -7.7180e-01,  2.9251e-01,\n",
       "         -6.0606e-01, -1.5136e+00, -2.7143e+00, -4.1164e-01, -1.2273e+00,\n",
       "         -4.1746e-01,  1.5021e+00],\n",
       "        [-6.2849e-01, -4.4247e-01,  5.6885e-01,  1.2803e+00, -5.5397e-01,\n",
       "          1.1179e+00, -6.0053e-01, -5.8619e-01, -2.8277e-01,  5.3390e-01,\n",
       "         -9.9388e-01, -1.6996e+00,  1.8362e+00,  4.2016e-01, -6.8729e-01,\n",
       "         -3.5060e-01,  7.5598e-01, -9.3632e-01, -8.4109e-02, -1.6361e+00,\n",
       "          1.0224e+00,  1.0733e+00, -5.7453e-01,  4.9668e-02,  7.2379e-01,\n",
       "          5.9746e-01,  2.6966e+00],\n",
       "        [ 2.7930e+00, -2.2745e+00, -2.3912e-01,  8.7498e-02,  1.4967e+00,\n",
       "         -5.7016e-01, -5.7248e-01,  1.9909e+00, -7.4416e-01,  7.2960e-01,\n",
       "          6.4083e-01,  1.6075e+00, -8.8810e-01,  2.7359e-01, -1.3257e-01,\n",
       "          1.2710e+00,  1.7234e+00,  1.1180e-01,  2.6952e-01,  1.1835e+00,\n",
       "          1.2575e+00,  1.3969e-01,  4.7259e-01,  7.9025e-01,  1.0811e+00,\n",
       "         -9.1965e-01, -4.0503e-01],\n",
       "        [ 4.5696e-01, -5.4184e-01, -2.3025e+00,  2.0127e+00, -4.6452e-01,\n",
       "         -5.8270e-01,  2.0863e+00, -4.7729e-02, -4.4920e-01,  9.5566e-01,\n",
       "         -1.4708e-01, -1.2532e+00, -1.1850e+00,  3.6583e-01, -1.4049e-01,\n",
       "          3.5252e-01, -5.2400e-01, -6.2844e-01, -9.3792e-01,  1.6772e+00,\n",
       "          3.8554e-03, -7.3685e-01, -9.3514e-01,  1.0465e-01, -4.6464e-01,\n",
       "          1.6676e+00,  1.3931e+00],\n",
       "        [ 6.5398e-01, -2.2449e-01,  1.2831e+00, -9.1787e-01, -3.3916e-01,\n",
       "         -1.8058e+00,  6.0518e-01, -5.6252e-01, -7.8933e-01,  1.2767e+00,\n",
       "         -1.0143e+00,  4.1611e-01, -7.5348e-01,  1.7128e+00, -8.7554e-01,\n",
       "          3.9714e-01,  8.4326e-01,  3.7988e-01, -1.1670e+00,  5.5228e-01,\n",
       "         -1.0279e+00, -3.9554e-01, -7.1410e-01, -8.7456e-02, -3.3361e-01,\n",
       "         -1.8798e-01, -1.2647e+00],\n",
       "        [ 2.0021e+00, -2.3470e-01, -1.3765e+00,  9.3426e-01,  1.0880e+00,\n",
       "          1.9179e-01,  3.0114e-01,  8.9896e-01, -8.4454e-01,  2.3267e-01,\n",
       "         -3.9205e-01, -2.5081e-01,  8.7124e-02,  1.3769e+00, -8.3358e-01,\n",
       "         -8.9400e-01,  1.1744e+00, -6.0779e-01, -1.1493e-01, -7.8077e-01,\n",
       "          1.9660e+00,  6.1175e-01,  3.6039e-01, -1.0274e+00,  1.1495e+00,\n",
       "          4.5111e-01,  6.4420e-01],\n",
       "        [ 2.1635e-01, -7.8731e-01, -3.3005e-01,  3.2877e-01, -1.6332e+00,\n",
       "          1.0807e+00,  3.3638e-01,  1.1536e-01,  3.2834e-01,  5.3447e-02,\n",
       "          1.4224e+00, -8.3957e-01, -2.4956e-01, -8.9778e-01, -8.6583e-01,\n",
       "         -1.0786e+00, -1.8384e-01,  7.1622e-01,  1.8175e-01,  1.1053e+00,\n",
       "          1.7003e+00, -1.6965e-01,  1.6293e-01,  1.3413e+00, -2.6301e-01,\n",
       "         -7.5521e-01,  8.1911e-01],\n",
       "        [ 7.4140e-01, -5.8787e-01, -4.6505e-01,  5.3112e-02,  2.2190e+00,\n",
       "         -3.5158e-01,  3.6381e-01,  2.5769e+00,  1.4544e+00, -6.1003e-01,\n",
       "         -5.9961e-01, -5.8392e-01, -1.8104e-02, -9.5177e-01, -9.6400e-01,\n",
       "         -2.8183e-01,  1.0597e+00, -7.2370e-01,  1.4755e-01, -3.2667e-01,\n",
       "          2.4958e+00,  1.1088e+00, -8.5476e-01,  1.8443e+00, -1.3881e-01,\n",
       "          1.3096e+00, -2.5802e-01],\n",
       "        [ 1.0669e+00,  2.1363e-01, -7.6603e-01, -1.6977e+00, -1.5023e-01,\n",
       "         -5.2150e-01, -6.3730e-01,  2.6214e-01,  7.6539e-03,  1.3067e+00,\n",
       "         -6.3482e-01, -1.1042e-04, -6.6158e-01,  1.4723e-01, -6.6036e-02,\n",
       "          5.2851e-01,  5.7950e-01,  2.1438e-01,  9.2200e-01,  5.2919e-01,\n",
       "          7.7070e-01,  4.2899e-01,  3.4330e-01,  2.0698e+00,  1.3405e+00,\n",
       "         -2.1746e-01,  8.6273e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    ix = 0\n",
    "    res = []\n",
    "    while True:\n",
    "        #============= Before ==============\n",
    "        p = N[ix].float()\n",
    "        p /= p.sum()\n",
    "    \n",
    "\n",
    "        #============= Now =================\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        res.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break   \n",
    "    print(''.join(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
